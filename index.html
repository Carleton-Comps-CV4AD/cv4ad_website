<html>
<head>
    <title>CV4AD</title>
    <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="css/style.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@200&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="style.css">
    <script src="script.js"></script>
</head>

<body>

    <div class = "titleGroup">
        <h1 class = "titles">CV4AD</h1>
        <h3 class = "subTitle">Computer Vision For Autonomous Driving</h3>
        <h3 class = "subTitle"><a class = "returnButton2" target="_blank" rel="noopener noreferrer" href ="https://www.linkedin.com/in/li-nathaniel/">Nathaniel Li</a> &nbsp &nbsp &nbsp <a class = "returnButton2" target="_blank" rel="noopener noreferrer" href="https://zehoubenzhao.com/">Ben Zhao</a> &nbsp &nbsp &nbsp <a class = "returnButton2" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/ethan-masadde/">Ethan Massade</a> &nbsp &nbsp &nbsp <a class = "returnButton2" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/josh-meier-00851b251/">Josh Meier</a> &nbsp &nbsp &nbsp <a class = "returnButton2" target="_blank" rel="noopener noreferrer" href = "https://www.linkedin.com/in/julian-tanguma-373605215/">Julian Tanguma</a> &nbsp &nbsp &nbsp <a class = "returnButton2" target="_blank" rel="noopener noreferrer" href ="https://toledod.github.io/">David Toledo</a>
        </h3>
    </div>
        <div class="navbar">
            <nav>
                <a href="https://github.com/Carleton-Comps-CV4AD" target="_blank" rel="noopener noreferrer">Github</a>
                
                <a href="javascript:void(0);" onclick="email();" target="_blank" rel="noopener noreferrer">Data</a>
                
                <a href="./presentation.html">Presentation</a>
            </nav>
        </div>
    <div class = "gifHolder">
        <img src="./images/gifs/video_49_clear_day.gif" alt="video of clear day simulation in CARLA">
    </div>
    <div class = "textHolder">
        <h3>Abstract</h3>
        <p class = "fillerText">Autonomous vehicles rely on computer vision to perceive their environment and operate safely within it, but the effects of weather and lighting such as rain, fog, and night can significant impact the performance of vision systems. For reliable integration into traffic, autonomous vehicles’ computer vision must be robust to the varying effects of weather. We measured the effects of weather on semantic segmentation and tracking models with simulated data, and then implemented three approaches to mitigate the effects of weather: domain adaptation with fine tuning, de-weathering model input, and multimodal sensor fusion. We collected image and lidar data on city driving scenes in the CARLA simulator across four scenarios, clear day, foggy day, rainy day, and clear night. After obtaining baseline performance for models trained on each of these scenarios, we evaluated our mitigation strategies. We show improvements in cross-domain performance for each of these methods and compare the merits and demerits of each approach.</p>
        <h3>Data</h3>
        <p class = "fillerText">For our project, our team used the CARLA simulator to generate an extensive dataset, which we split into training, validation, and test sets. The training and validation data were used to train our neural network and create weights for our models, which then processed the test data to produce segmentation masks and corresponding performance scores. Initially, CARLA provided a simple route featuring basic instances of vehicles and pedestrians. However, to meet the data-hungry demands of neural networks, we expanded our dataset to include diverse weather conditions—such as fog, rain, and nighttime—and collaborated with the instance segmentation and semantic segmentation teams to further augment our data with images, videos, and other data such as lidar data. This comprehensive effort resulted in the creation of over one million data files that significantly advanced our project.
        </p>
        <h3_1>Semantic Segmentation</h3_1>
        <figure class = "imageHolder">
            <figcaption1>Left to Right: RGB, Ground Truth, Prediction</figcaption1>
            <img src="./images/imgs/0.png" alt="semantic segmentation output clear day">
            <figcaption>Clear Day</figcaption>
        </figure>
        <figure class = "imageHolder">
            <img src="./images/imgs/0foggy.png" alt="semantic segmentation output clear day">
            <figcaption>Foggy Day</figcaption>
        </figure>
        <figure class = "imageHolder">
            <img src="./images/imgs/0night.png" alt="semantic segmentation output clear day">
            <figcaption2>Clear Night</figcaption2>
            
        </figure>
        <p class = "fillerText">For our project, our team utilized semantic segmentation to analyze how different weather conditions 
            affect the ability of autonomous vehicles to detect objects and parse their environments. 
            Using a model adapted from MIT"S ADE20k dataset in PyTorch, our approach aimed ot benchmark performance across 4 distinct weatheer condtiions. 
            The model utilized an HRNETV2 archetecture with a single convolution module as the decoder to maintain high resolution representations of the data.
            The model was trained on a single NVIDIA GPU. Our approach was able to achieve a mean IOU of 0.65 on baseline data, a significant improvement over the original source code. 
        </p>
        <h3>Instance Segmentation</h3>
        <p class = "fillerText">As an extension on the task of semantic segmenation, we also trained a video instance segmentation model on CARLA data to see how weather would affect its ability to distinguish between multiple instances of the same types of objects and keep track of them between the frames of a video. We utilized the MMDetection framework and the MaskTrackRCNN model, which is typically trained on the large YoutubeVIS2021 dataset, to accomplish this task. Our training was done on one Nvidia GPU on each of the 4 weather types. We generally saw lower performance than the original implementation, but we were able to still show the significant differences in model performance when tested on weathers that each model was not trained on.</p>
        <h3>Measured Effects of Weather</h3>
        <p class = "fillerText">filler text </p>
        <h3>Measured Mitigated Effects </h3>
        <p class = "fillerText">To address the problem of the weather, we decided to try three different solutions: Domain Adaptation, Deweathering, and Sensor Fusion. After generating new data sets for each solution, we applied them to our models to obtain new metrics to compare to our previous results. Domain adaptation had the best results out of the three, but all showed some signs of helping our weathering problem.</p>
    </div>
</body>

<div class = "gifHolder">
   <iframe width="1200" height="600" src="https://www.youtube.com/embed/NepRPRpq_i4?si=RgdeLRZud2XCf-3z" title="CV4AD Presentation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

<div class = "textHolder">
    <p class = "fillerText2"> Special thank you to,
        <br>
         our advisor, <a class = "returnButton2" href = "https://www.cs.carleton.edu/faculty/tamert/" target="_blank" rel="noopener noreferrer">Tanya Amert</a> <br>
         and to <a class = "returnButton2" href = "https://people.carleton.edu/~mtie/index.html" target="_blank" rel="noopener noreferrer">Mike Tie</a>.
</p>

</div>

</html>
